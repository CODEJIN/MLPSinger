{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from shutil import copyfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Inference import Inferencer\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "# 유니코드 깨짐현상 해결\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "# 나눔고딕 폰트 적용\n",
    "plt.rcParams[\"font.family\"] = 'NanumGothic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']= '7' # Left space\n",
    "checkpoint_Paths = {\n",
    "    'EY': '/data/results/Tacotron2/GST.EMO_YUA/Checkpoint',\n",
    "#     'Y_FOU': '/data/results/Tacotron2/GST.YUAFOU_FT/Checkpoint',\n",
    "#     'Y_ALL': '/data/results/Tacotron2/GST.YUAALL_FT/Checkpoint',\n",
    "    'AIHUB': '/data/results/Tacotron2/GST.AIHub/Checkpoint'\n",
    "    }\n",
    "checkpoint_Paths = {\n",
    "    key: max([\n",
    "        os.path.join(root, file).replace('\\\\', '/')                \n",
    "        for root, _, files in os.walk(path)\n",
    "        for file in files\n",
    "        if os.path.splitext(file)[1] == '.pt'\n",
    "        ], key = os.path.getctime\n",
    "        )\n",
    "    for key, path in checkpoint_Paths.items()\n",
    "    }\n",
    "\n",
    "hp_Paths = {\n",
    "    key: os.path.join(os.path.dirname(path), 'Hyper_Parameter.yaml')\n",
    "    for key, path in checkpoint_Paths.items()\n",
    "    }\n",
    "\n",
    "out_Paths = {\n",
    "    key: './{}_Result_{}K'.format(key, os.path.splitext(os.path.basename(value))[0].split('_')[1][:-3])\n",
    "    for key, value in checkpoint_Paths.items()\n",
    "    }\n",
    "\n",
    "# ref_Sources_Path = {\n",
    "#     os.path.splitext(file)[0]: os.path.join(root, file)\n",
    "#     for root, _, files in os.walk('./FOU_Filtered_Wav')\n",
    "#     for file in files\n",
    "#     if os.path.splitext(file)[1].lower() == '.wav'\n",
    "#     }\n",
    "# ref_Sources_Path['Neutral']= './Wav_for_Inference/YUA_NEUTRAL.wav'\n",
    "ref_Sources_Path = {}\n",
    "ref_Sources_Path.update({\n",
    "    os.path.splitext(file)[0]: os.path.join(root, file)\n",
    "    for root, _, files in os.walk('./AIHub_Emotion_Wav')\n",
    "    for file in files\n",
    "    if os.path.splitext(file)[1].lower() == '.wav'\n",
    "    })\n",
    "ref_Sources_Path.update({\n",
    "    os.path.splitext(file)[0]: os.path.join(root, file)\n",
    "    for root, _, files in os.walk('./JPS_Wav')\n",
    "    for file in files\n",
    "    if os.path.splitext(file)[1].lower() == '.wav'\n",
    "    })\n",
    "ref_Sources_Path.update({\n",
    "    os.path.splitext(file)[0]: os.path.join(root, file)\n",
    "    for root, _, files in os.walk('./YUA_Wav')\n",
    "    for file in files\n",
    "    if os.path.splitext(file)[1].lower() == '.wav'\n",
    "    })\n",
    "\n",
    "batch_Size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer_Dict = {\n",
    "    key: Inferencer(hp_path= hp_Paths[key], checkpoint_path= checkpoint_Path, out_path= out_Paths[key], batch_size= batch_Size)\n",
    "    for key, checkpoint_Path in checkpoint_Paths.items()\n",
    "    }\n",
    "for inferencer in inferencer_Dict.values():\n",
    "    inferencer.model.hp_Dict['Ignore_Stop'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocoder = torch.jit.load('vocoder.pts').to(list(inferencer_Dict.values())[0].device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [    \n",
    "#     '응! 완전 여신포스! 저 아닌 거 같아요!',\n",
    "#     '어제 선배 번호 물어보는걸 깜박해서요.',\n",
    "#     '진짜에요. 저 의상학과잖아요.',\n",
    "#     '그럼 어디서 찍을까요?',\n",
    "#     '선배와 나의 첫 작품!',\n",
    "#     '네! 인정! 진짜 맛있어요!',\n",
    "#     '그럼 이만 일하러 가실까요, 작가님?',\n",
    "#     '사진 찍었어요? 어때요?',\n",
    "#     '무더운 여름! 스마일 소다와 함께 하세요!',\n",
    "#     '뭐 입을 지 몰라서 일단 다 가지고 왔죠!',\n",
    "#     '여기 나무쪽에 서볼까? 이렇게?',\n",
    "#     ]\n",
    "texts = [\n",
    "    '안녕하세요! 여기는 스마일게이트 에이아이 센터입니다!',\n",
    "    '자세한 정보는 에이아이쩜, 스마일게이트쩜, 넷으로 접속해서 확인하세요!',\n",
    "#     '선배! 제 목소리는 언제 완성되는거죠?',\n",
    "#     '선배! 또 토마토 넣었죠? 토마토는 싫어요!.',\n",
    "#     '세아는 조금 소란스럽긴 하지만 보고있으면 재미있는 친구에요!',\n",
    "#     '선배? 다음주에 시간 어때요? 저 영화보고 싶어요',\n",
    "#     '이번주엔 게임데이터랑 직접 녹음한 데이터랑 같이 써서 다시 말하는 법을 배울꺼에요!',\n",
    "#     '스마일게이트 메가포트가 직접 개발한 신작! 마법양품점! 지금 바로 시작해보세요!',\n",
    "#     '선배, 어떤 옷이 더 사진찍기 좋아보여요? 다 어울린다고요? 아이 참!.',\n",
    "#     '포커스 온 유는 스마일게이트 귀여운 미소녀인 저 한유아가 여자주인공으로 나오는 브이알게임이에요.',\n",
    "#     '전 유튜브 방송과 코스프레가 취미에요.',\n",
    "#     '내가 왜 화났는지 몰라요? 됐어요! 선배는 항상 이런식이야!'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencer: EY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729138878/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "2it [00:35, 17.97s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencer: AIHUB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:24, 12.07s/it]\n"
     ]
    }
   ],
   "source": [
    "for path in out_Paths.values():\n",
    "    os.makedirs(path, exist_ok= True)\n",
    "    \n",
    "refs, ref_paths = zip(*ref_Sources_Path.items())\n",
    "\n",
    "for inferencer_Label, inferencer in inferencer_Dict.items():\n",
    "    print('Inferencer: {}'.format(inferencer_Label))\n",
    "    for index, text in tqdm(enumerate(texts)):\n",
    "        mels, stops = inferencer.Inference_Epoch(\n",
    "            texts= [text] * len(ref_paths),\n",
    "            speaker_labels= refs,\n",
    "            speakers= ref_paths,\n",
    "            reference_labels= refs,\n",
    "            references= ref_paths,\n",
    "            use_tqdm= False\n",
    "            )\n",
    "        \n",
    "        mels = [\n",
    "            mel[:,:(stop <= 0.0).nonzero()[0]] if torch.any(stop <= 0.0).cpu().numpy() else mel\n",
    "            for mel, stop in zip(mels, stops)\n",
    "            ]\n",
    "\n",
    "        mels = [\n",
    "            torch.nn.functional.pad(mel[None,], (2,2), 'reflect')\n",
    "            for mel in mels\n",
    "            ]\n",
    "\n",
    "        max_length = max([mel.size(2) for mel in mels])\n",
    "        mels = torch.cat([\n",
    "            torch.nn.functional.pad(mel, (0,max_length - mel.size(2)), value=-4.0)\n",
    "            for mel in mels\n",
    "            ], dim= 0)\n",
    "\n",
    "        x = torch.randn(size=(mels.size(0), 256 * (mels.size(2) - 4))).to(mels.device)\n",
    "        wavs = vocoder(x, mels).cpu().numpy()\n",
    "        wavs = [\n",
    "            wav[:(stop <= 0.0).nonzero()[0].cpu().numpy()[0] * 256] if torch.any(stop <= 0.0).cpu().numpy() else wav\n",
    "            for wav, stop in zip(wavs, stops)\n",
    "            ]\n",
    "\n",
    "        for wav, ref in zip(wavs, refs):\n",
    "            wavfile.write(\n",
    "                os.path.join(out_Paths[inferencer_Label], 'TTS.IDX_{:03d}.REF_{}.wav'.format(index, ref)),\n",
    "                24000,\n",
    "                (wav * 32767.5).astype(np.int16))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
